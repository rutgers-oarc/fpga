
## References - Links

- course : https://www.intel.com/content/www/us/en/programmable/support/training/course/iopncl2102day.html
- Set Up Guide
- Getting Started Guide
- Best Practices Guide

## Labs description and lessons learned

### Lab 1

- setup and usage of aoc compiler  and aocl utility
- eclipse for editing and building the project (and debugging if you know how to use Eclipse for that)
- look at the html reports generated by the compiler - in particular the Summary tab and Loop Analysis tab
- look at II in the report. You want to have ~1 (II to be about 1, which indicates loops well unrolled)
- how the main.cpp looks like - this is host code - much of it is boilerplate, so good for copy-pasting. The real work is in optimizing the kernel. 

```
aocl compile-config
aocl link-config
aoc -march=emulator summation.cl
aoc -rtl -board=a10gx summation.cl
firefox summation/reports/report.html & 
``` 

- use `#pragma unroll` to replicate loop in hardware

### Lab 2 - Optimizing IIR Single Work-Item kernel 

- how to take your CPU code and refactor it so that part of it runs on the accelerator. In this case all it took was copy-paste and remove pointer-ing (this is a good tip - avoid pointers, work on arrays instead. Compiler will thank you.)
- looking at Area Analysis of Source tab in the html report 
- one single division that was inside the loop - taking it out of the loop and turning it into multiplication gave a great improvement
- using compiler flags like `fpc` and `-fp-relaxed` improved performance (`fpc` skip rounding floating points at each stage of calculation, and round at the end instead. `fp-relaxed` relax reordering rules for FP clacs in order to use associative property)
- show II decrease if you use ints instead of floats


### Lab 3 - Practice With Channels

- create channels - i.e. going between different kernels on the board so that you can take advantage of FPGA-type parallelization of loop pipelining (needs explanation, this is a key concept)
- each channel needs its own queue
- you need some synchronization to occur (I don't understand this fully)


```
        //Create Command queue
        //TODO: Add command queues
        cl::CommandQueue queue1(mycontext, DeviceList[0]); 
        cl::CommandQueue queue2(mycontext, DeviceList[0]); 
        cl::CommandQueue queue3(mycontext, DeviceList[0]); 
        assert(err==CL_SUCCESS);

        //Create Buffers for input and output
        cl::Buffer Buffer_In(mycontext, CL_MEM_READ_ONLY, sizeof(cl_float)*vectorSize);
        cl::Buffer Buffer_Out(mycontext, CL_MEM_WRITE_ONLY, sizeof(cl_float)*vectorSize);

        err = queue1.enqueueWriteBuffer(Buffer_In, CL_FALSE, 0, sizeof(cl_float)*vectorSize, X);

        //TODO: change the name of the .aocx file
        std::ifstream aocx_stream("channels.aocx", std::ios::in|std::ios::binary);
        checkErr(aocx_stream.is_open() ? CL_SUCCESS:-1, "channels.aocx");
        std::string prog(std::istreambuf_iterator<char>(aocx_stream), (std::istreambuf_iterator<char>()));
        cl::Program::Binaries mybinaries (1, std::make_pair(prog.c_str(), prog.length()+1));

        // Create the Program from the AOCX file.
        cl::Program program(mycontext, DeviceList, mybinaries);


        //TODO: Create new kernels
        cl::Kernel host_reader_kernel(program, "host_reader", &err);
        cl::Kernel process_data_kernel(program, "process_data", &err);
        cl::Kernel host_writer_kernel(program, "host_writer", &err);

        //TODO: Set arguments for all kernels
        err = host_reader_kernel.setArg(0, Buffer_In);
        assert(err==CL_SUCCESS);
        err = host_reader_kernel.setArg(1, vectorSize); 

        // TODO: launch additional kernels
        err=queue1.enqueueTask(host_reader_kernel);

        //TODO: Wait on all the queues
        queue1.finish();

```

in kernel: 

```
#pragma OPENCL EXTENSION cl_intel_channels : enable
channel float c0 __attribute__((depth(128)));
read_channel_intel(c1);
write_channel_intel(c1, idata);
```

- always use `restrict` keyword when you can - it indicates no dependency
- 


### Lab 4

Showed how to read profile reports - in the case of multiplying 1M x 1M matrices - going from 3 minutes (for naive CPU implementation) to 63 miliseconds by changing a few lines of code and defining workgroups (see code). Also 

profiler report: 
```
aocl report matrixMult.aocs profile.mon matrixMult.source
```

```
#pragma unroll
#define BLOCK_SIZE 64 
__attribute((reqd_work_group_size(BLOCK_SIZE,BLOCK_SIZE,1)))
__attribute((num_compute_units(2)))
__attribute((num_simd_work_items(2)))
```


### Lab 5 - Local Memory Optimizations

This lab showed how to write code so that compiler creates the local memory blocks with no stalls. 

- use powers of 2 for your indices for your arrays (even if that looks like wasting resources - 2048 is much better than 2000)
- use ```__attribute__(bankbits( 13, 12, 11))``` with local/private arrays - this produces the design where no stalls are there
- index your multidimensional arrays so that small array sizes are the last index, not the first - ```int a[2048][20]``` is much better than ```int a[20][2048]```


### Lab 6

## Material covered

### Day 1 


### Day 2 

- Workgroups 
- Channels

### Day 3

Memory - how to write your code so that communication time is decreased, by using memory on chip, or 
directing compiler to create memory block on the workgroups

## Other stuff

Useful Analogy about work groups and work items

| factory analogy | FPGA term |
|-----------------|-----------|
| factory	| board |
| work function 	|   kernel |
| worker  person | work-item |
| build unit (body building, engine building|  workgroup |

